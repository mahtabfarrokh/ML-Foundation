{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "**What does it do?**\n",
    "A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It works by learning simple decision rules inferred from the data features. Think of it like a flowchart where each internal node represents a decision on a feature, each branch is the outcome of that decision, and each leaf node represents a final prediction (class or value).\n",
    "\n",
    "**How it works (Classification example):**\n",
    "\n",
    "1. Pick the best feature to split the data. This is based on metrics like Gini Impurity, Entropy, or Information Gain.\n",
    "\n",
    "2. Split the dataset based on that feature.\n",
    "\n",
    "3. Repeat this process recursively for each child node until:\n",
    "\n",
    "- All data points in a node belong to the same class, or\n",
    "\n",
    "- A stopping condition is met (e.g. max depth, min samples).\n",
    "\n",
    "- Assign class labels to leaf nodes (majority vote).\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- Gini Impurity: Measures how ‚Äúmixed‚Äù the classes are in a node. Lower is better.\n",
    "\n",
    "- Entropy: A measure of disorder. Decision Trees using entropy try to maximize information gain.\n",
    "\n",
    "- Information Gain: The reduction in entropy before vs after a split.\n",
    "\n",
    "- Overfitting: Trees can grow too deep and perfectly fit the training data, hurting generalization. Use pruning or set depth limits.\n",
    "\n",
    "- Pruning: Cutting back the tree to prevent overfitting (pre-pruning or post-pruning).\n",
    "\n",
    "**Decision Tree Characteristics:**\n",
    "\n",
    "- Interpretable: Easy to visualize and understand.\n",
    "\n",
    "- Non-parametric: No assumptions about data distribution.\n",
    "\n",
    "- Can handle both numerical and categorical features.\n",
    "\n",
    "- Sensitive to small changes in data (can lead to different splits).\n",
    "\n",
    "\n",
    "\n",
    "**Popular Variants:**\n",
    "\n",
    "- Random Forests ‚Äì ensemble of Decision Trees for better performance.\n",
    "\n",
    "- Gradient Boosted Trees ‚Äì builds trees sequentially, each correcting the last.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Alt text](../images/decision_tree_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implmentation without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and test\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Node\n",
    "class Node:\n",
    "    def __init__(self, feature=None, value=None, left=None, right=None, label=None):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Calculate Gini Impurity\n",
    "def gini(y):\n",
    "    classes = np.unique(y)\n",
    "    probs = [(np.sum(y == c) / len(y)) for c in classes]\n",
    "    return 1 - sum(p**2 for p in probs)\n",
    "\n",
    "\n",
    "# Calculate Entropy Impurity\n",
    "def entropy(y): \n",
    "    classes = np.unique(y)\n",
    "    probs = [(np.sum(y == c) / len(y)) for c in classes]\n",
    "    return  - np.sum(probs * np.log(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Split data by feature and value\n",
    "def split(X, y, feature, value):\n",
    "    left = X[:, feature] <= value\n",
    "    right = X[:, feature] > value\n",
    "    return X[left], y[left], X[right], y[right]\n",
    "\n",
    "# Find best split\n",
    "def best_split(X, y, method=\"gini\"):\n",
    "    best_score, best_feat, best_val = 1, None, None\n",
    "    for feat in range(X.shape[1]):\n",
    "        for val in np.unique(X[:, feat]):\n",
    "            X_l, y_l, X_r, y_r = split(X, y, feat, val)\n",
    "            if len(y_l) == 0 or len(y_r) == 0:\n",
    "                continue\n",
    "            if method == \"gini\": \n",
    "                score = (len(y_l)*gini(y_l) + len(y_r)*gini(y_r)) / len(y)\n",
    "            else: \n",
    "                score = (len(y_l)*entropy(y_l) + len(y_r)*entropy(y_r)) / len(y)\n",
    "            if score < best_score:\n",
    "                best_score, best_feat, best_val = score, feat, val\n",
    "    return best_feat, best_val\n",
    "\n",
    "# Get majority label\n",
    "def majority_label(y):\n",
    "    labels, counts = np.unique(y, return_counts=True)\n",
    "    return labels[np.argmax(counts)]\n",
    "\n",
    "# Build tree\n",
    "def build_tree(X, y, depth=0, max_depth=3):\n",
    "    if len(np.unique(y)) == 1 or depth == max_depth:\n",
    "        return Node(label=majority_label(y))\n",
    "    feat, val = best_split(X, y, method=\"entropy\")\n",
    "    if feat is None:\n",
    "        return Node(label=majority_label(y))\n",
    "    X_l, y_l, X_r, y_r = split(X, y, feat, val)\n",
    "    return Node(feature=feat, value=val,\n",
    "                left=build_tree(X_l, y_l, depth+1, max_depth),\n",
    "                right=build_tree(X_r, y_r, depth+1, max_depth))\n",
    "\n",
    "# Predict one sample\n",
    "def predict_one(x, node):\n",
    "    if node.label is not None:\n",
    "        return node.label\n",
    "    if x[node.feature] <= node.value:\n",
    "        return predict_one(x, node.left)\n",
    "    else:\n",
    "        return predict_one(x, node.right)\n",
    "\n",
    "# Predict many\n",
    "def predict(tree, X):\n",
    "    return [predict_one(x, tree) for x in X]\n",
    "\n",
    "\n",
    "\n",
    "tree = build_tree(X_train, y_train, max_depth=3)\n",
    "y_pred = predict(tree, X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≤ Random Forest Steps: \n",
    "\n",
    "1. Bootstrap Sampling: Randomly sample (with replacement) from the training data to create multiple datasets.\n",
    "\n",
    "2. Build Decision Trees\n",
    "\n",
    "3. Train a decision tree on each bootstrap sample.\n",
    "\n",
    "    - At each split, only a random subset of features is considered (adds randomness and reduces overfitting).\n",
    "\n",
    "    - Trees are typically grown deep (not pruned), making them strong individual classifiers.\n",
    "\n",
    "4. Aggregate Predictions\n",
    "\n",
    "    - Classification: majority vote across trees\n",
    "\n",
    "    - Regression: average the outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Random Forest): 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Random Forest: build multiple trees on random subsets of data and features\n",
    "import random\n",
    "\n",
    "def bootstrap_sample(X, y):\n",
    "    indices = np.random.choice(len(X), len(X), replace=True)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "def random_forest(X, y, n_trees=5, max_depth=3):\n",
    "    trees = []\n",
    "    for _ in range(n_trees):\n",
    "        X_sample, y_sample = bootstrap_sample(X, y)\n",
    "        tree = build_tree(X_sample, y_sample, max_depth=max_depth)\n",
    "        trees.append(tree)\n",
    "    return trees\n",
    "\n",
    "def predict_forest(trees, X):\n",
    "    tree_preds = np.array([predict(tree, X) for tree in trees])\n",
    "    final_preds = []\n",
    "    for i in range(X.shape[0]):\n",
    "        vals, counts = np.unique(tree_preds[:, i], return_counts=True)\n",
    "        final_preds.append(vals[np.argmax(counts)])\n",
    "    return np.array(final_preds)\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "forest = random_forest(X_train, y_train, n_trees=10, max_depth=3)\n",
    "y_forest_pred = predict_forest(forest, X_test)\n",
    "print(\"Accuracy (Random Forest):\", accuracy_score(y_test, y_forest_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Gradient Boosting\n",
    "\n",
    "**üéØ Key Idea**: Build the model sequentially, each step reducing the error from the previous step.\n",
    "\n",
    "**Steps**: \n",
    "1. Start with a Weak Model: Usually a simple decision tree (a \"stump\") that makes an initial prediction, or a constant average value. 2.\n",
    "2. Repeat: \n",
    "    \n",
    "    1. Compute Residuals (Errors): Measure how far off the model‚Äôs predictions are from the actual targets.\n",
    "    \n",
    "    2. Train Next Tree on Residuals: A new tree is trained to predict the residuals (errors) ‚Äî i.e., where the last model failed.\n",
    "    \n",
    "    3. Update the Model: Add the new tree‚Äôs predictions to the previous ones, scaled by a learning rate (to control step size): F = F_old + $\\alpha$ * F_new\n",
    "\n",
    "Continue adding trees, each one correcting the mistakes of the combined ensemble so far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ XGBoost (Extreme Gradient Boosting)\n",
    "XGBoost is a fast, regularized, and optimized version of Gradient Boosting. \n",
    "\n",
    "It is designed for handling large and complex datasets. Has regularization and pruning\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Start with an initial prediction, often the mean of the target.\n",
    "2. Repeat:\n",
    "    1. Calculate the residuals = prediction - target\n",
    "    2. Calculate the similarity score = (sum of residuals)<sup>2</sup> / (N(residuals) + $\\lambda$)\n",
    "    3. Find the best split with the most gain: Gain = left_child_similarity + right_child_similarity - root_similarity\n",
    "    4. If the gain is more than gamma do the split. \n",
    "3. The finaloutput prediction is = (sum of residuals) / (N(residuals) + $\\lambda$)\n",
    "\n",
    "\n",
    "**Source**: StatQuest YouTube video: https://www.youtube.com/watch?v=OtD8wVaFm6E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° LightGBM\n",
    "LightGBM is a fast, efficient gradient boosting framework developed by Microsoft, optimized for speed and scalability.\n",
    "\n",
    "How it works: \n",
    "1. Histogram-based Binning\n",
    "LightGBM discretizes continuous features into bins (e.g., 255 bins). This speeds up training and reduces memory usage.\n",
    "\n",
    "2. Leaf-wise Tree Growth:\n",
    "LightGBM does not grow level-wise, instead, it chooses the leaf with the max loss (or residual) to split. So, it can grow deep, and unbalanced trees.\n",
    "It can overfit easily if you don't set a proper stopping condition for it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implmentation of decision tree with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train model\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
